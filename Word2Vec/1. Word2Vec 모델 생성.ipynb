{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "## 1. Word2Vec 모델 생성하기\n",
    "- Doc2Vec 모델 생성시 사용한 코퍼스 뭉치를 사용하여 Word2Vec 모델을 생성한다.\n",
    "- 작성 일시: 2018-06-12\n",
    "- 수정 일시: 2018-06-12\n",
    "- 작성자: 부현경 (hyunkyung.boo@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. DB로부터 코퍼스 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from konlpy.tag import Twitter\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from mysql.connector.errors import Error\n",
    "\n",
    "\n",
    "table_config = {\n",
    "    'user': 'root',\n",
    "    'password': '1234',\n",
    "    'host': 'localhost',\n",
    "    'port': 3306,\n",
    "    'database': 'db_test',\n",
    "    'raise_on_warnings': True,\n",
    "    'charset' : 'utf8'\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = mysql.connector.connect(**table_config)\n",
    "    curs = conn.cursor()\n",
    "\n",
    "    sql_select_data1 =  \"select idx, tokenized_user_review from naver_movie_info where data_type = 'Train' and tokenized_user_review != \\\"\\\"\"\n",
    "    sql_select_data2 =  \"select idx, tokenized_user_review from naver_movie_info where data_type = 'Validate' and tokenized_user_review != \\\"\\\"\"\n",
    "\n",
    "    df1 = pd.read_sql(sql_select_data1, con=conn, columns=True)\n",
    "    df2 = pd.read_sql(sql_select_data2, con=conn, columns=True)\n",
    "\n",
    "    print(\"데이터 프레임 변환 완료!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "finally:\n",
    "    conn.close()\n",
    "    \n",
    "# 모델 학습시 사용\n",
    "T_tokenized_df = df1['tokenized_user_review']\n",
    "# 모델 정확도 평가시 사용\n",
    "V_tokenized_df = df2['tokenized_user_review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word2Vec 모델 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# 모델 생성시 작업자의 경우 멀티프로세싱처리가 가능\n",
    "# import multiprocessing\n",
    "# 'workers': multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 리스트로 읽기\n",
    "def readRows(df):\n",
    "    sentences = []\n",
    "    for row in df.iteritems():\n",
    "         if row[1] != '':\n",
    "            sentences.append([w for w in row[1].split(', ')])\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Word2Vec 학습모델 생성\n",
    "def create_word2vec_model(sentences_vocab, config):\n",
    "    start = time.time()\n",
    "    model = Word2Vec(**config)\n",
    "    model.build_vocab(sentences_vocab)\n",
    "    model.train(sentences_vocab, total_examples=len(sentences_vocab), epochs=model.iter)\n",
    "    end = time.time()\n",
    "    print(\"During Time: {}\".format(end - start))\n",
    "    return model\n",
    "\n",
    "\n",
    "# 저장 모델 로드\n",
    "def load_word2vec_model(model_path):\n",
    "    model = Word2Vec.load(model_path)\n",
    "    return model\n",
    "    \n",
    "\n",
    "# multi-configs에 따른 모델 생성 함수\n",
    "# 다른 parameter 수정시에는 코드를 일부 수정해야 한다.\n",
    "# config에 따른 모델 생성 + 저장을 한꺼번에 한다.\n",
    "def getModels(sentences_vocab, s_list, w_list, e_list, mc_list):\n",
    "    cnt = 0\n",
    "    for s in s_list:\n",
    "        for w in w_list:\n",
    "            for e in e_list:\n",
    "                for mc in mc_list:\n",
    "                    config = {\n",
    "                                'sg': 1,\n",
    "                                'window': w,  # distance between the predicted word and context words\n",
    "                                'size': s,  # vector size\n",
    "                                'batch_words': 10000,\n",
    "                                'epochs ': e,  # 보통 딥러닝에서 말하는 epoch과 비슷한, 반복 횟수\n",
    "                                'min_count': mc,  # ignore with freq lower\n",
    "                                'workers': 1,  # multi cpu. 1이 속도가 느리지만 그나마.. 메모리를 적게 먹는다.\n",
    "                                }\n",
    "\n",
    "                    cnt += 1\n",
    "                    label = \"s{0}_w{1}_e{2}_mc{3}\".format(str(s), str(w), str(e), str(mc))\n",
    "                    name = \"W2V_{0}_setting_{1}.model\".format(cnt, label)\n",
    "                    model = create_word2vec_model(sentences_vocab, config)\n",
    "                    save_root = \"D:\\Word2Vec_model_20180612\\\\\"\n",
    "                    model_save_path = save_root + name\n",
    "                    model.save(model_save_path)\n",
    "                    print(\"저장 완료! \", \"저장 위치:\", model_save_path)\n",
    "                    model.init_sims(replace=True)\n",
    "\n",
    "\n",
    "\n",
    "# sentences_train = readRows(T_tokenized_df)\n",
    "\n",
    "# # 9 * 5 * 3 * 8 =  1,080\n",
    "# size = [5, 10, 15, 20, 40, 60, 100, 200, 300]\n",
    "# window = [1, 3, 5, 8, 10]\n",
    "# iter_count = [3, 5, 10]\n",
    "# min_count = [1, 2, 3, 4, 5, 10, 20, 50]\n",
    "\n",
    "# # 모델 생성&저장\n",
    "# getModels(sentences_train, size, window, iter_count, min_count)\n",
    "# # print(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences_train = readRows(T_tokenized_df)\n",
    "\n",
    "config = {\n",
    "            'sg': 1,\n",
    "            'window': 10,  # distance between the predicted word and context words\n",
    "            'size': 200,  # vector size\n",
    "            'batch_words': 10000,\n",
    "            'iter': 10,  # 보통 딥러닝에서 말하는 epoch과 비슷한, 반복 횟수\n",
    "            'min_count': 100,  # ignore with freq lower\n",
    "            'workers': 1,  # multi cpu. 1이 속도가 느리지만 그나마.. 메모리를 적게 먹는다.\n",
    "        }\n",
    "\n",
    "name = \"W2V.model\"\n",
    "model = create_word2vec_model(sentences_train, config)\n",
    "save_root = \"D:\\Word2Vec_model_20180612\\\\\"\n",
    "model_save_path = save_root + name\n",
    "model.save(model_save_path)\n",
    "print(\"저장 완료! \", \"저장 위치:\", model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences_train = readRows(T_tokenized_df)\n",
    "\n",
    "config = {\n",
    "            'sg': 1,\n",
    "            'window': 10,  # distance between the predicted word and context words\n",
    "            'size': 300,  # vector size\n",
    "            'batch_words': 10000,\n",
    "            'iter': 10,  # 보통 딥러닝에서 말하는 epoch과 비슷한, 반복 횟수\n",
    "            'min_count': 100,  # ignore with freq lower\n",
    "            'workers': 1,  # multi cpu. 1이 속도가 느리지만 그나마.. 메모리를 적게 먹는다.\n",
    "        }\n",
    "\n",
    "name = \"W2V2.model\"\n",
    "model = create_word2vec_model(sentences_train, config)\n",
    "save_root = \"D:\\Word2Vec_model_20180612\\\\\"\n",
    "model_save_path = save_root + name\n",
    "model.save(model_save_path)\n",
    "print(\"저장 완료! \", \"저장 위치:\", model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences_train = readRows(T_tokenized_df)\n",
    "\n",
    "config = {\n",
    "            'sg': 1,\n",
    "            'window': 10,  # distance between the predicted word and context words\n",
    "            'size': 200,  # vector size\n",
    "            'batch_words': 10000,\n",
    "            'iter': 10,  # 보통 딥러닝에서 말하는 epoch과 비슷한, 반복 횟수\n",
    "            'min_count': 50,  # ignore with freq lower\n",
    "            'workers': 1,  # multi cpu. 1이 속도가 느리지만 그나마.. 메모리를 적게 먹는다.\n",
    "        }\n",
    "\n",
    "name = \"W2V3.model\"\n",
    "model = create_word2vec_model(sentences_train, config)\n",
    "save_root = \"D:\\Word2Vec_model_20180612\\\\\"\n",
    "model_save_path = save_root + name\n",
    "model.save(model_save_path)\n",
    "print(\"저장 완료! \", \"저장 위치:\", model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences_train = readRows(T_tokenized_df)\n",
    "\n",
    "config = {\n",
    "            'sg': 1,\n",
    "            'window': 10,  # distance between the predicted word and context words\n",
    "            'size': 200,  # vector size\n",
    "            'batch_words': 10000,\n",
    "            'iter': 10,  # 보통 딥러닝에서 말하는 epoch과 비슷한, 반복 횟수\n",
    "            'min_count': 1,  # ignore with freq lower\n",
    "            'workers': 1,  # multi cpu. 1이 속도가 느리지만 그나마.. 메모리를 적게 먹는다.\n",
    "        }\n",
    "\n",
    "name = \"W2V4.model\"\n",
    "model = create_word2vec_model(sentences_train, config)\n",
    "save_root = \"D:\\Word2Vec_model_20180612\\\\\"\n",
    "model_save_path = save_root + name\n",
    "model.save(model_save_path)\n",
    "print(\"저장 완료! \", \"저장 위치:\", model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences_train = readRows(T_tokenized_df)\n",
    "\n",
    "config = {\n",
    "            'sg': 1,\n",
    "            'window': 10,  # distance between the predicted word and context words\n",
    "            'size': 200,  # vector size\n",
    "            'batch_words': 10000,\n",
    "            'iter': 15,  # 보통 딥러닝에서 말하는 epoch과 비슷한, 반복 횟수\n",
    "            'min_count': 1,  # ignore with freq lower\n",
    "            'workers': 1,  # multi cpu. 1이 속도가 느리지만 그나마.. 메모리를 적게 먹는다.\n",
    "        }\n",
    "\n",
    "name = \"W2V5.model\"\n",
    "model = create_word2vec_model(sentences_train, config)\n",
    "save_root = \"D:\\Word2Vec_model_20180612\\\\\"\n",
    "model_save_path = save_root + name\n",
    "model.save(model_save_path)\n",
    "print(\"저장 완료! \", \"저장 위치:\", model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-12 18:42:55,147 : INFO : loading Word2Vec object from D:\\Word2Vec_model_20180612\\W2V.model\n",
      "2018-06-12 18:42:55,166 : INFO : loading wv recursively from D:\\Word2Vec_model_20180612\\W2V.model.wv.* with mmap=None\n",
      "2018-06-12 18:42:55,167 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-12 18:42:55,168 : INFO : loading vocabulary recursively from D:\\Word2Vec_model_20180612\\W2V.model.vocabulary.* with mmap=None\n",
      "2018-06-12 18:42:55,169 : INFO : loading trainables recursively from D:\\Word2Vec_model_20180612\\W2V.model.trainables.* with mmap=None\n",
      "2018-06-12 18:42:55,171 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-12 18:42:55,171 : INFO : loaded D:\\Word2Vec_model_20180612\\W2V.model\n",
      "2018-06-12 18:42:55,174 : INFO : loading Word2Vec object from D:\\Word2Vec_model_20180612\\W2V2.model\n",
      "2018-06-12 18:42:55,208 : INFO : loading wv recursively from D:\\Word2Vec_model_20180612\\W2V2.model.wv.* with mmap=None\n",
      "2018-06-12 18:42:55,211 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-12 18:42:55,215 : INFO : loading vocabulary recursively from D:\\Word2Vec_model_20180612\\W2V2.model.vocabulary.* with mmap=None\n",
      "2018-06-12 18:42:55,219 : INFO : loading trainables recursively from D:\\Word2Vec_model_20180612\\W2V2.model.trainables.* with mmap=None\n",
      "2018-06-12 18:42:55,220 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-12 18:42:55,223 : INFO : loaded D:\\Word2Vec_model_20180612\\W2V2.model\n",
      "2018-06-12 18:42:55,226 : INFO : loading Word2Vec object from D:\\Word2Vec_model_20180612\\W2V3.model\n",
      "2018-06-12 18:42:55,289 : INFO : loading wv recursively from D:\\Word2Vec_model_20180612\\W2V3.model.wv.* with mmap=None\n",
      "2018-06-12 18:42:55,289 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-12 18:42:55,292 : INFO : loading vocabulary recursively from D:\\Word2Vec_model_20180612\\W2V3.model.vocabulary.* with mmap=None\n",
      "2018-06-12 18:42:55,296 : INFO : loading trainables recursively from D:\\Word2Vec_model_20180612\\W2V3.model.trainables.* with mmap=None\n",
      "2018-06-12 18:42:55,296 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-12 18:42:55,297 : INFO : loaded D:\\Word2Vec_model_20180612\\W2V3.model\n",
      "2018-06-12 18:42:55,305 : INFO : loading Word2Vec object from D:\\Word2Vec_model_20180612\\W2V4.model\n",
      "2018-06-12 18:42:55,763 : INFO : loading wv recursively from D:\\Word2Vec_model_20180612\\W2V4.model.wv.* with mmap=None\n",
      "2018-06-12 18:42:55,767 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-12 18:42:55,769 : INFO : loading vocabulary recursively from D:\\Word2Vec_model_20180612\\W2V4.model.vocabulary.* with mmap=None\n",
      "2018-06-12 18:42:55,772 : INFO : loading trainables recursively from D:\\Word2Vec_model_20180612\\W2V4.model.trainables.* with mmap=None\n",
      "2018-06-12 18:42:55,773 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-12 18:42:55,774 : INFO : loaded D:\\Word2Vec_model_20180612\\W2V4.model\n",
      "2018-06-12 18:42:55,854 : INFO : loading Word2Vec object from D:\\Word2Vec_model_20180612\\W2V5.model\n",
      "2018-06-12 18:42:56,564 : INFO : loading wv recursively from D:\\Word2Vec_model_20180612\\W2V5.model.wv.* with mmap=None\n",
      "2018-06-12 18:42:56,566 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-12 18:42:56,567 : INFO : loading vocabulary recursively from D:\\Word2Vec_model_20180612\\W2V5.model.vocabulary.* with mmap=None\n",
      "2018-06-12 18:42:56,567 : INFO : loading trainables recursively from D:\\Word2Vec_model_20180612\\W2V5.model.trainables.* with mmap=None\n",
      "2018-06-12 18:42:56,569 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-12 18:42:56,571 : INFO : loaded D:\\Word2Vec_model_20180612\\W2V5.model\n",
      "2018-06-12 18:42:56,664 : INFO : estimated required memory for 1304 words and 200 dimensions: 2738400 bytes\n",
      "2018-06-12 18:42:56,665 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-06-12 18:42:56,671 : INFO : estimated required memory for 1304 words and 300 dimensions: 3781600 bytes\n",
      "2018-06-12 18:42:56,671 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-06-12 18:42:56,678 : INFO : estimated required memory for 2245 words and 200 dimensions: 4714500 bytes\n",
      "2018-06-12 18:42:56,680 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-06-12 18:42:56,686 : INFO : estimated required memory for 32293 words and 200 dimensions: 67815300 bytes\n",
      "2018-06-12 18:42:56,687 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-06-12 18:42:56,748 : INFO : estimated required memory for 32293 words and 200 dimensions: 67815300 bytes\n",
      "2018-06-12 18:42:56,750 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'vocab': 652000, 'vectors': 1043200, 'syn1neg': 1043200, 'total': 2738400}\n",
      "1 [('스릴러', 0.4708366394042969), ('멜로', 0.4661795496940613), ('공포영화', 0.37364476919174194)]\n",
      "1 [('공포영화', 0.5705064535140991), ('스릴러', 0.49120429158210754), ('무섭다', 0.4490625858306885)]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "2 {'vocab': 652000, 'vectors': 1564800, 'syn1neg': 1564800, 'total': 3781600}\n",
      "2 [('멜로', 0.46827614307403564), ('스릴러', 0.4577628970146179), ('공포영화', 0.3604389727115631)]\n",
      "2 [('공포영화', 0.5403527617454529), ('스릴러', 0.4729056656360626), ('깜짝', 0.4339043200016022)]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "3 {'vocab': 1122500, 'vectors': 1796000, 'syn1neg': 1796000, 'total': 4714500}\n",
      "3 [('멜로', 0.4420172870159149), ('스릴러', 0.40668985247612), ('장르', 0.38747286796569824)]\n",
      "3 [('공포영화', 0.5405879020690918), ('공포물', 0.5004182457923889), ('미스테리', 0.49281299114227295)]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "4 {'vocab': 16146500, 'vectors': 25834400, 'syn1neg': 25834400, 'total': 67815300}\n",
      "4 [('멜로', 0.5289991497993469), ('스릴러', 0.5242161154747009), ('환타지', 0.4890875816345215)]\n",
      "4 [('호러물', 0.6619588136672974), ('오컬트', 0.6457652449607849), ('공포영화', 0.6327605247497559)]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "5 {'vocab': 16146500, 'vectors': 25834400, 'syn1neg': 25834400, 'total': 67815300}\n",
      "5 [('멜로', 0.4549548029899597), ('스릴러', 0.44366252422332764), ('환타지', 0.43623465299606323)]\n",
      "5 [('공포영화', 0.6093379259109497), ('호러물', 0.5985567569732666), ('오컬트', 0.5831844806671143)]\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "# 파일 읽기\n",
    "model1 = load_word2vec_model(\"D:\\Word2Vec_model_20180612\\W2V.model\")\n",
    "model2 = load_word2vec_model(\"D:\\Word2Vec_model_20180612\\W2V2.model\")\n",
    "model3 = load_word2vec_model(\"D:\\Word2Vec_model_20180612\\W2V3.model\")\n",
    "model4 = load_word2vec_model(\"D:\\Word2Vec_model_20180612\\W2V4.model\")\n",
    "model5 = load_word2vec_model(\"D:\\Word2Vec_model_20180612\\W2V5.model\")\n",
    "\n",
    "\n",
    "models = [model1, model2, model3, model4, model5]\n",
    "\n",
    "cnt = 0\n",
    "for model in models:\n",
    "    cnt += 1\n",
    "    print(cnt, model.estimate_memory())\n",
    "    print(cnt, model.wv.most_similar(positive=['공포', '로맨스'], negative=['재미있다'], topn=3))\n",
    "    print(cnt, model.wv.most_similar('공포', topn=3))\n",
    "    print(\"---------------------------------------------------------------------------------------------------\")\n",
    "# model.wv.log_accuracy('D:\\\\Word2Vec_model_20180612\\\\validationSet.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim \n",
    "import gensim.models as g\n",
    "\n",
    "# 폰트 설정\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "font_name = mpl.font_manager.FontProperties(fname='C:/Windows/Fonts/malgun.ttf').get_name()\n",
    "mpl.rc('font', family=font_name)\n",
    "# print (plt.rcParams['font.family'] )\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    vocab = list(model.wv.vocab)\n",
    "    X = model.wv[vocab]\n",
    "    # print(len(model.wv.vocab.keys()))\n",
    "    # print(len(X))\n",
    "    # print(X[0][:10])\n",
    "    tsne = TSNE(n_components=2)\n",
    "\n",
    "    # 100개의 단어에 대해서만 시각화\n",
    "    X_tsne = tsne.fit_transform(X[:150])\n",
    "    # X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "    df = pd.DataFrame(X_tsne, index=vocab[:150], columns=['x', 'y'])\n",
    "    df.shape\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(40,20)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    ax.scatter(df['x'], df['y'])\n",
    "\n",
    "    for word, pos in df.iterrows():\n",
    "        ax.annotate(word, pos, fontsize=30)\n",
    "    plt.show()\n",
    "    print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 단어장 및 모델 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Word2Vec 학습 모델 갱신\n",
    "def update_word2vec_model(model, sentences_updated):\n",
    "    model.build_vocab(sentences_updated, update=True)\n",
    "    model.train(sentences_updated, total_examples=len(sentences_updated), epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "\n",
    "sentences_train = readRows(T_tokenized_df)\n",
    "sentences_add = readRows(V_tokenized_df)\n",
    "sentences_new= sentences_train + sentences_add\n",
    "\n",
    "model = load_word2vec_model(\"D:\\Word2Vec_model_20180612\\W2V4.model\")\n",
    "model = update_word2vec_model(model, sentences_new)\n",
    "save_root = \"D:\\Word2Vec_model_20180612\\\\\"\n",
    "model_save_path = save_root + 'update_W2V.model'\n",
    "model.save(model_save_path)\n",
    "print(\"저장 완료! \", \"저장 위치:\", model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-12 18:46:24,828 : INFO : estimated required memory for 35335 words and 200 dimensions: 74203500 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab': 17667500, 'vectors': 28268000, 'syn1neg': 28268000, 'total': 74203500}\n",
      "[('무속신앙', 0.42442092299461365), ('스릴러', 0.4134938716888428), ('환타지', 0.4115058183670044), ('멜로', 0.40173518657684326), ('호러물', 0.39570868015289307), ('월씬', 0.3922324776649475), ('매개', 0.39122796058654785), ('공포영화', 0.3878144323825836), ('옥죄', 0.3769144117832184), ('호러', 0.3763198256492615)]\n",
      "[('공포영화', 0.6274300217628479), ('무속신앙', 0.6033227443695068), ('스텐바이미', 0.5815481543540955), ('넘쳣', 0.5712491273880005), ('드래그미투헬', 0.5686925649642944), ('감관', 0.5659478902816772), ('옥죄', 0.5618089437484741), ('월씬', 0.5522574186325073), ('호러물', 0.5518032312393188), ('뻐근하네', 0.5506236553192139)]\n"
     ]
    }
   ],
   "source": [
    "print(model.estimate_memory())\n",
    "print(model.wv.most_similar(positive=['공포', '로맨스'], negative=['재미있다'], topn=10))\n",
    "print(model.wv.most_similar('공포', topn=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
